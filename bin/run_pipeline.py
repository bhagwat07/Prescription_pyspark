import osimport get_all_variables as gavfrom create_object import get_spark_objectimport loggingimport logging.configimport sysfrom pres_run_data_ingest import load_filefrom validations import df_count,df_top10_rec,df_print_schemafrom pres_run_data_preprossing import perform_data_cleanfrom presc_run_data_transform import city_report, top_5_prescriberfrom data_store import extract_filelogging.config.fileConfig(fname='../utils/logging_to_log.conf')def main():    #get spark object    try:        logging.info('main method Start')        spark = get_spark_object(gav.envn,gav.appName)        print(spark)        #load the city file.        for file in os.listdir(gav.stagging_dim_city):            print('File is ' + file)            file_dir = gav.stagging_dim_city + '\\' + file            print(file_dir)            if file.split('.')[1]=='parquet':                file_format='parquet'                header='NA'                inferSchema='NA'            elif file.split('.')[1]=='csv':                file_format='csv'                header = gav.header                inferSchema=gav.inferSchema        df_city = load_file(spark=spark, file_dir=file_dir, file_format=file_format, header=header,inferSchema=inferSchema)        #load the Presc_medicare_dataframe file        for file in os.listdir(gav.stagging_fact):            print('File is ' + file)            file_dir = gav.stagging_fact + '\\' + file            print(file_dir)            if file.split('.')[1]=='parquet':                file_format='parquet'                header='NA'                inferSchema='NA'            elif file.split('.')[1]=='csv':                file_format='csv'                header = gav.header                inferSchema=gav.inferSchema        df_fact = load_file(spark=spark,file_dir=file_dir,file_format=file_format,header=header,inferSchema=inferSchema)        #validate the df_city        # df_count(df_city,'df_city')        # df_top10_rec(df_city,'df_city')        #validate the df_fact        # df_count(df_fact,'df_fact')        # df_top10_rec(df_fact,'df_fact')        #load the df_city_sel to perform some transformation.        df_city_sel,df_fact_sel = perform_data_clean(df_city,df_fact)        #validate the transformed data frame.        df_top10_rec(df_city_sel,'df_city_sel')        df_top10_rec(df_fact_sel,'df_fact_sel')        #Checking the printSchema.        df_print_schema(df_city_sel,'df_city_sel')        df_print_schema(df_fact_sel,'df_fact_sel')        #Initiative presc_run_data_transform script        df_city_final = city_report(df_city_sel,df_fact_sel)        df_presc_final = top_5_prescriber(df_fact_sel)        # validate by printing Schema        df_top10_rec(df_city_final,'df_city_final')        df_print_schema(df_city_final,'df_city_final')        df_top10_rec(df_presc_final, 'df_presc_final')        df_print_schema(df_presc_final, 'df_presc_final')        #storing the output.        city_path = gav.city_storing_path        extract_file(df_city_final,1,'json',city_path,'False','snappy')        fact_path = gav.fact_storing_path        extract_file(df_presc_final,2,'orc',fact_path,'False','snappy')        logging.info("run_pipeline is completed\n")    except Exception as exp:        logging.error('Error Occured in main method. Please check the Stack trace to go the respective module' + str(exp),exc_info=True)        sys.exit(1)if __name__ == '__main__':    logging.info("presc Data Pipeline is running\n")    main()