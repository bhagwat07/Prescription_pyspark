import loggingimport logging.configfrom pyspark.sql.functions import upper,lit,regexp_extract,col,concat_ws,count,when,isnan,coalesce,round,avgfrom pyspark.sql.window import Windowlogging.config.fileConfig(fname='../utils/logging_to_log.conf')logger = logging.getLogger('pres_run_data_preprossing.py')def perform_data_clean(df1,df2):    #clean the dataframe.    #select only required field from dataframe.    #convert some required field into upper case.    try:        logger.info('perform_data_clean() is started for df_city .....')        df_city_sel = df1.select(upper(df1.city).alias('city'),                                 df1.state_id,                                 upper(df1.state_name).alias('state_name'),                                 upper(df1.county_name).alias('county_name'),                                 df1.population,                                 df1.zips)        #clean df_fact dataframe.        #1 select only required column        #2 rename the column.        logger.info('perform_data_clean() is started for df_fact .....')        df_fact_sel = df2.select(df2.npi.alias('presc_id'),                                 df2.nppes_provider_last_org_name.alias('presc_lname'),                                 df2.nppes_provider_first_name.alias('presc_fname'),                                 df2.nppes_provider_city.alias('presc_city'),                                 df2.nppes_provider_state.alias('presc_state'),                                 df2.specialty_description.alias('presc_spclt'),                                 df2.years_of_exp,df2.drug_name,                                 df2.total_claim_count.alias('trx_cnt'),                                 df2.total_day_supply,df2.total_drug_cost)        #3 Adding a country Field 'USA'        df_fact_sel = df_fact_sel.withColumn('country_name',lit('USA'))        df_fact_sel = df_fact_sel.withColumn('presc_id',col('presc_id').cast('int'))        #4 cleaning year_of_experience field =45.        pattern = '\d+'        idx = 0        df_fact_sel = df_fact_sel.withColumn('years_of_exp',regexp_extract('years_of_exp',pattern,idx))        #5 convert the years_of_exp string datatype to integerDataType.        df_fact_sel = df_fact_sel.withColumn('years_of_exp',col('years_of_exp').cast('int'))        #6 combine the first_name and last_name        df_fact_sel = df_fact_sel.withColumn('presc_fullname',concat_ws('_',col('presc_fname'),col('presc_lname'))).drop('presc_fname','presc_lname')        #7 checking and cleaning null value        # df_fact_sel.select([count(when(col(c).isNull() | isnan(c),c)).alias(c)   for c in df_fact_sel.columns]).show()        #8 delete the record where presc_id and drug_name is Null.        df_fact_sel = df_fact_sel.dropna(subset=['presc_id','drug_name'])        #9 Impute the TRX_CNT where it is null as avg of trx_cnt of that prescriber.        win = Window.partitionBy('presc_id')        df_fact_sel = df_fact_sel.withColumn('trx_cnt',coalesce('trx_cnt',round(avg('trx_cnt').over(win))))        df_fact_sel = df_fact_sel.withColumn('trx_cnt',col('trx_cnt').cast('int'))        df_fact_sel = df_fact_sel.withColumn('presc_id',col('presc_id').cast('int'))        #check the null value once again.        # df_fact_sel.select([count(when(col(c).isNull() | isnan(c), c)).alias(c) for c in df_fact_sel.columns]).show()    except Exception as exp:        logger.error('Error in perform_data_clean() method. Please check the Stack Trace ', str(exp))        raise    else:        logger.info('perform_data_clean operation run successfully....')    return df_city_sel,df_fact_sel